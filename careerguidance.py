# -*- coding: utf-8 -*-
"""careerguidance.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16ZwFRRW_I4XTbFWs0aOzD3nxhDo61vPR
"""

# -------------------------------
# IMPORTS FOR STREAMLIT WEB APP
# -------------------------------
import streamlit as st

# -------------------------------
# DATA HANDLING
# -------------------------------
import pandas as pd
import numpy as np

# -------------------------------
# VISUALIZATION
# -------------------------------
import matplotlib.pyplot as plt
import seaborn as sns

# -------------------------------
# MACHINE LEARNING (Sklearn)
# -------------------------------
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, classification_report

# -------------------------------
# SYSTEM / WARNINGS (Optional)
# -------------------------------
import warnings
warnings.filterwarnings("ignore")



df = pd.read_csv('student_career_guidance_dataset_1000.csv')


df.columns

df.dtypes

df.shape

df.size

df.describe()

df.describe(include="object")

df.head()

df.tail()

df.isnull().sum()

df.duplicated().sum()

df.head(2)

df['Interests'].value_counts()

plt.figure(figsize=(7, 4))
sns.countplot(data=df, x='Strengths', palette='viridis', hue='Strengths', legend=False)
plt.title('Distribution of Student Strengths')
plt.xlabel('Strengths')
plt.ylabel('Number of Students')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

df['Skills'].value_counts()

df['Personality'].value_counts()

plt.figure(figsize=(7, 4))
sns.countplot(data=df, x='Skills', palette='viridis', hue='Skills', legend=False)
plt.title('Distribution of Student Skills')
plt.xlabel('Skills')
plt.ylabel('Number of Students')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

plt.figure(figsize=(7, 4))
sns.countplot(data=df, x='Personality', palette='viridis', hue='Personality', legend=False)
plt.title('Distribution of Student Personality Types')
plt.xlabel('Personality')
plt.ylabel('Number of Students')
plt.tight_layout()
plt.show()

df['Goals'].value_counts()

plt.figure(figsize=(7, 4))
sns.countplot(data=df, x='Goals', palette='viridis', hue='Goals', legend=False)
plt.title('Distribution of Student Goals')
plt.xlabel('Goals')
plt.ylabel('Number of Students')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

df['Budget'].value_counts()

plt.figure(figsize=(7, 4))
sns.countplot(data=df, x='Budget', palette='viridis', hue='Budget', legend=False)
plt.title('Distribution of Student Budget Categories')
plt.xlabel('Budget')
plt.ylabel('Number of Students')
plt.tight_layout()
plt.show()

df['Suggested_Career'].value_counts()

plt.figure(figsize=(7, 4))
sns.countplot(data=df, x='Suggested_Career', palette='viridis', hue='Suggested_Career', legend=False)
plt.title('Distribution of Suggested Careers')
plt.xlabel('Suggested Career')
plt.ylabel('Number of Students')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

df['Recommended_Course'].value_counts()

plt.figure(figsize=(7, 4))
sns.countplot(data=df, x='Recommended_Course', palette='viridis', hue='Recommended_Course', legend=False)
plt.title('Distribution of Recommended Course')
plt.xlabel('Recommended Course')
plt.ylabel('Number of Students')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

df['Scholarship_Option'].value_counts()

plt.figure(figsize=(7, 4))
sns.countplot(data=df, x='Scholarship_Option', palette='viridis', hue='Scholarship_Option', legend=False)
plt.title('Distribution of Scholarship Option')
plt.xlabel('Scholarship Option')
plt.ylabel('Number of Students')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

print(df['Marks'].describe())
plt.figure(figsize=(7, 4))
sns.histplot(data=df, x='Marks', kde=True, palette='viridis')
plt.title('Distribution of Student Marks')
plt.xlabel('Marks')
plt.ylabel('Number of Students')
plt.tight_layout()
plt.show()

plt.figure(figsize=(8, 5))
sns.boxplot(data=df, x='Suggested_Career', y='Marks', palette='viridis', hue='Suggested_Career', legend=False)
plt.title('Distribution of Marks by Suggested Career')
plt.xlabel('Suggested Career')
plt.ylabel('Marks')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Calculate the frequency of each skill within each suggested career
skill_career_crosstab = pd.crosstab(df['Suggested_Career'], df['Skills'])
st.dataframe(skill_career_crosstab)

# Plotting the stacked bar chart
plt.figure(figsize=(8, 5))
skill_career_crosstab.plot(kind='bar', stacked=True, colormap='viridis', ax=plt.gca())
plt.title('Distribution of Skills by Suggested Career')
plt.xlabel('Suggested Career')
plt.ylabel('Number of Students')
plt.xticks(rotation=45, ha='right')
plt.legend(title='Skills', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

# Calculate the frequency of each goal within each suggested career
goal_career_crosstab = pd.crosstab(df['Suggested_Career'], df['Goals'])
st.dataframe(goal_career_crosstab)

# Plotting the stacked bar chart
plt.figure(figsize=(8, 5))
goal_career_crosstab.plot(kind='bar', stacked=True, colormap='viridis', ax=plt.gca())
plt.title('Distribution of Goals by Suggested Career')
plt.xlabel('Suggested Career')
plt.ylabel('Number of Students')
plt.xticks(rotation=45, ha='right')
plt.legend(title='Goals', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

# Calculate the frequency of each interest within each suggested career
interest_career_crosstab = pd.crosstab(df['Suggested_Career'], df['Interests'])
st.dataframe(interest_career_crosstab)

# Plotting the stacked bar chart
plt.figure(figsize=(8, 5))
interest_career_crosstab.plot(kind='bar', stacked=True, colormap='viridis', ax=plt.gca())
plt.title('Distribution of Interests by Suggested Career')
plt.xlabel('Suggested Career')
plt.ylabel('Number of Students')
plt.xticks(rotation=45, ha='right')
plt.legend(title='Interests', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

# Calculate the frequency of each personality type within each suggested career
personality_career_crosstab = pd.crosstab(df['Suggested_Career'], df['Personality'])
st.dataframe(personality_career_crosstab)

# Plotting the stacked bar chart
plt.figure(figsize=(8, 5))
personality_career_crosstab.plot(kind='bar', stacked=True, colormap='viridis', ax=plt.gca())
plt.title('Distribution of Personality Types by Suggested Career')
plt.xlabel('Suggested Career')
plt.ylabel('Number of Students')
plt.xticks(rotation=45, ha='right')
plt.legend(title='Personality', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()



# Calculate the frequency of each personality type within each suggested career
personality_career_crosstab = pd.crosstab(df['Suggested_Career'], df['Personality'])
st.dataframe(personality_career_crosstab)

# Plotting the stacked bar chart
plt.figure(figsize=(8,5))
personality_career_crosstab.plot(kind='bar', stacked=True, colormap='viridis', ax=plt.gca())
plt.title('Distribution of Personality Types by Suggested Career')
plt.xlabel('Suggested Career')
plt.ylabel('Number of Students')
plt.xticks(rotation=45, ha='right')
plt.legend(title='Personality', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

# Calculate the frequency of each personality type within each suggested career
personality_career_crosstab = pd.crosstab(df['Suggested_Career'], df['Personality'])
st.dataframe(personality_career_crosstab)

# Plotting the stacked bar chart
plt.figure(figsize=(8, 5))
personality_career_crosstab.plot(kind='bar', stacked=True, colormap='viridis', ax=plt.gca())
plt.title('Distribution of Personality Types by Suggested Career')
plt.xlabel('Suggested Career')
plt.ylabel('Number of Students')
plt.xticks(rotation=45, ha='right')
plt.legend(title='Personality', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

budget_career_crosstab = pd.crosstab(df['Suggested_Career'], df['Budget'])
st.dataframe(budget_career_crosstab)

plt.figure(figsize=(8, 5))
budget_career_crosstab.plot(kind='bar', stacked=True, colormap='viridis', ax=plt.gca())
plt.title('Distribution of Budget Categories by Suggested Career')
plt.xlabel('Suggested Career')
plt.ylabel('Number of Students')
plt.xticks(rotation=45, ha='right')
plt.legend(title='Budget', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

from sklearn.model_selection import GridSearchCV

# Define the parameter grid to search
param_grid = {
    'n_estimators': [50, 100, 200],  # Number of trees in the forest
    'max_depth': [None, 10, 20],   # Maximum depth of the tree
    'min_samples_split': [2, 5, 10], # Minimum number of samples required to split an internal node
    'min_samples_leaf': [1, 2, 4]    # Minimum number of samples required to be at a leaf node
}

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42), # Use the same base model
                           param_grid=param_grid,
                           cv=3, # 3-fold cross-validation
                           n_jobs=-1, # Use all available cores
                           verbose=2, # Output progress
                           scoring='f1_weighted') # Metric to optimize

# Fit GridSearchCV to the training data
grid_search.fit(X_train, y_train)

print("Best parameters found: ", grid_search.best_params_)
print("Best F1-weighted score: ", grid_search.best_score_)

# Get the best model
best_model = grid_search.best_estimator_

# Make predictions on the test set with the best model
y_pred_tuned = best_model.predict(X_test)

# Evaluate the tuned model
accuracy_tuned = accuracy_score(y_test, y_pred_tuned)
report_tuned = classification_report(y_test, y_pred_tuned)

print(f"\nTuned Model Accuracy: {accuracy_tuned:.4f}")
print("\nTuned Classification Report:\n", report_tuned)

categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
categorical_cols.remove('Suggested_Career')

print("Categorical columns to be encoded:", categorical_cols)

from sklearn.model_selection import train_test_split

# Apply one-hot encoding to the identified categorical columns
df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)

# Define features (X) and target (y)
X = df_encoded.drop(columns=['Student_ID', 'Suggested_Career'])
y = df_encoded['Suggested_Career']

# Split the data into training and testing sets (70-30 ratio, random state for reproducibility)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

print(f"Shape of X_train: {X_train.shape}")
print(f"Shape of X_test: {X_test.shape}")
print(f"Shape of y_train: {y_train.shape}")
print(f"Shape of y_test: {y_test.shape}")

from sklearn.ensemble import RandomForestClassifier

# Instantiate a RandomForestClassifier model with a random_state for reproducibility
model = RandomForestClassifier(random_state=42)

# Train the model using the fit method with X_train and y_train
model.fit(X_train, y_train)

print("RandomForestClassifier model trained successfully.")

from sklearn.metrics import classification_report, accuracy_score

# Make predictions on the test set
y_pred = model.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Model Accuracy: {accuracy:.4f}")

# Generate classification report
report = classification_report(y_test, y_pred)
print("Classification Report:\n", report)

import xgboost as xgb
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import LabelEncoder

# Initialize LabelEncoder
label_encoder = LabelEncoder()

# Fit LabelEncoder on y_train and transform both y_train and y_test
y_train_encoded = label_encoder.fit_transform(y_train)
y_test_encoded = label_encoder.transform(y_test)

# Initialize XGBClassifier
# Use the encoded labels for num_class
xgb_model = xgb.XGBClassifier(objective='multi:softmax', num_class=len(label_encoder.classes_),
                            random_state=42, use_label_encoder=False, eval_metric='mlogloss')

# Train the model with encoded labels
xgb_model.fit(X_train, y_train_encoded)

# Make predictions on the test set with encoded labels
y_pred_xgb_encoded = xgb_model.predict(X_test)

# Decode predictions back to original labels for the classification report
y_pred_xgb = label_encoder.inverse_transform(y_pred_xgb_encoded)

# Evaluate the XGBoost model using the original y_test labels for comparison
accuracy_xgb = accuracy_score(y_test, y_pred_xgb)
report_xgb = classification_report(y_test, y_pred_xgb, target_names=label_encoder.classes_)

print(f"XGBoost Model Accuracy: {accuracy_xgb:.4f}")
print("\nXGBoost Classification Report:\n", report_xgb)



import streamlit as st
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# Load the dataset
@st.cache_data
def load_data():
    df = pd.read_csv('/content/student_career_guidance_dataset_1000.csv')
    return df

df = load_data()

# --- Feature Engineering and Model Training ---

# Identify categorical columns for encoding (excluding Student_ID and Suggested_Career)
categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
if 'Suggested_Career' in categorical_cols:
    categorical_cols.remove('Suggested_Career')

# Apply one-hot encoding
df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)

# Define features (X) and target (y)
X = df_encoded.drop(columns=['Student_ID', 'Suggested_Career'])
y = df_encoded['Suggested_Career']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train the RandomForestClassifier with best parameters found by GridSearchCV
# Best parameters: {'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100}
model = RandomForestClassifier(
    n_estimators=100,
    max_depth=None,
    min_samples_split=2,
    min_samples_leaf=2,
    random_state=42
)
model.fit(X_train, y_train)

# --- Streamlit Application ---
st.title('Student Career Guidance Predictor')
st.write('Enter student attributes to predict the suggested career.')

# Create input widgets for user
# For simplicity, we list all unique values for dropdowns.
# In a real app, these might come from a configuration or further processing.

interests_options = df['Interests'].unique()
strengths_options = df['Strengths'].unique()
skills_options = df['Skills'].unique()
personality_options = df['Personality'].unique()
goals_options = df['Goals'].unique()
budget_options = df['Budget'].unique()
recommended_course_options = df['Recommended_Course'].unique()
scholarship_option_options = df['Scholarship_Option'].unique()

with st.sidebar:
    st.header('Student Input Features')
    marks = st.slider('Marks', 50, 100, 75)
    interests = st.selectbox('Interests', interests_options)
    strengths = st.selectbox('Strengths', strengths_options)
    skills = st.selectbox('Skills', skills_options)
    personality = st.selectbox('Personality', personality_options)
    goals = st.selectbox('Goals', goals_options)
    budget = st.selectbox('Budget', budget_options)
    recommended_course = st.selectbox('Recommended Course', recommended_course_options)
    scholarship_option = st.selectbox('Scholarship Option', scholarship_option_options)

# Create a DataFrame for the user's input
user_input = pd.DataFrame({
    'Marks': [marks],
    'Interests': [interests],
    'Strengths': [strengths],
    'Skills': [skills],
    'Personality': [personality],
    'Goals': [goals],
    'Budget': [budget],
    'Recommended_Course': [recommended_course],
    'Scholarship_Option': [scholarship_option]
})

# Apply one-hot encoding to user input, ensuring all columns match the training data
# This step is critical to ensure the input DataFrame has the same columns as X_train
user_encoded = pd.get_dummies(user_input, columns=categorical_cols, drop_first=True)

# Align columns - add missing columns with 0 and remove extra columns
missing_cols = set(X_train.columns) - set(user_encoded.columns)
for c in missing_cols:
    user_encoded[c] = 0
user_encoded = user_encoded[X_train.columns]

# Make prediction
if st.button('Predict Suggested Career'):
    prediction = model.predict(user_encoded)
    st.success(f"The Suggested Career is: **{prediction[0]}**")

st.write("--- ")
st.write("**Note:** The model's accuracy is low (around 13%). The predictions should be interpreted with caution.")



